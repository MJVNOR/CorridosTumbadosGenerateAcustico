{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoMusicGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJVNOR/DatasetMidiCorridosTumbadosAcustico/blob/main/AutoMusicGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVeNGstkTYKu",
        "outputId": "19ee0b82-4b35-4924-b236-af7e28b739a6"
      },
      "source": [
        "!git clone \"https://github.com/MJVNOR/DatasetMidiCorridosTumbadosAcustico.git\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DatasetMidiCorridosTumbadosAcustico'...\n",
            "remote: Enumerating objects: 506, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 506 (delta 2), reused 0 (delta 0), pack-reused 463\u001b[K\n",
            "Receiving objects: 100% (506/506), 251.69 MiB | 17.62 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (298/298), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzwIIyqQRvQd"
      },
      "source": [
        "#library for understanding music\n",
        "from music21 import *\n",
        "import pickle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZrwFXIjR-0V"
      },
      "source": [
        "#defining function to read MIDI files\n",
        "def read_midi(file):\n",
        "    \n",
        "    print(\"Loading Music File:\",file)\n",
        "    \n",
        "    notes=[]\n",
        "    notes_to_parse = None\n",
        "    \n",
        "    #parsing a midi file\n",
        "    midi = converter.parse(file)\n",
        "  \n",
        "    #grouping based on different instruments\n",
        "    s2 = instrument.partitionByInstrument(midi)\n",
        "\n",
        "    #Looping over all the instruments\n",
        "    for part in s2.parts:\n",
        "    \n",
        "        #select elements of only piano\n",
        "        if 'Piano' in str(part): \n",
        "        \n",
        "            notes_to_parse = part.recurse() \n",
        "      \n",
        "            #finding whether a particular element is note or a chord\n",
        "            for element in notes_to_parse:\n",
        "                \n",
        "                #note\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                \n",
        "                #chord\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    return np.array(notes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5D8HgnlSD52",
        "outputId": "ec23cef7-2acf-493c-f9bf-cdaca5c8a806"
      },
      "source": [
        "#for listing down the file names\n",
        "import os\n",
        "\n",
        "#Array Processing\n",
        "import numpy as np\n",
        "\n",
        "#specify the path\n",
        "path='/content/DatasetMidiCorridosTumbadosAcustico/Only5/'\n",
        "\n",
        "#read all the filenames\n",
        "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
        "\n",
        "#reading each midi file\n",
        "notes_array = np.array([read_midi(path+i) for i in files])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Music File: /content/DatasetMidiCorridosTumbadosAcustico/Only5/Junior H - No Me Pesa.mid\n",
            "Loading Music File: /content/DatasetMidiCorridosTumbadosAcustico/Only5/Junior H - Con Un Presidente.mid\n",
            "Loading Music File: /content/DatasetMidiCorridosTumbadosAcustico/Only5/Junior H - Clave Ali.mid\n",
            "Loading Music File: /content/DatasetMidiCorridosTumbadosAcustico/Only5/Junior H - Atrapado en un Sue√±o.mid\n",
            "Loading Music File: /content/DatasetMidiCorridosTumbadosAcustico/Only5/Junior H - Venadeando.mid\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOnAy7-MSIKe",
        "outputId": "b450c216-151e-4743-a19d-224fd7bf9baf"
      },
      "source": [
        "#converting 2D array into 1D array\n",
        "notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "with open('DatasetMidiCorridosTumbadosAcustico/DataNotes/notes_', 'wb') as filepath:\n",
        "        pickle.dump(notes_, filepath)\n",
        "\n",
        "#No. of unique notes\n",
        "unique_notes = list(set(notes_))\n",
        "print(len(unique_notes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "pk6LDe8vSH_i",
        "outputId": "1b623657-5bcb-4bb2-8541-b051f539a717"
      },
      "source": [
        "#importing library\n",
        "from collections import Counter\n",
        "\n",
        "#computing frequency of each note\n",
        "freq = dict(Counter(notes_))\n",
        "\n",
        "#library for visualiation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#consider only the frequencies\n",
        "no=[count for _,count in freq.items()]\n",
        "\n",
        "#set the figure size\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "#plot\n",
        "plt.hist(no)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([190.,  13.,   4.,   8.,   5.,   3.,   1.,   2.,   0.,   2.]),\n",
              " array([  1. ,  79.2, 157.4, 235.6, 313.8, 392. , 470.2, 548.4, 626.6,\n",
              "        704.8, 783. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 322,
              "height": 302
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy_yuVBxSQE7",
        "outputId": "16376bb7-93dd-4f2a-e3d9-b9d9539ac5e1"
      },
      "source": [
        "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "print(len(frequent_notes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3MvwzhDSTVi",
        "outputId": "04ddcb60-a794-4827-8873-c0d0bb2eb2df"
      },
      "source": [
        "new_music=[]\n",
        "\n",
        "for notes in notes_array:\n",
        "    temp=[]\n",
        "    for note_ in notes:\n",
        "        if note_ in frequent_notes:\n",
        "            temp.append(note_)            \n",
        "    new_music.append(temp)\n",
        "    \n",
        "new_music = np.array(new_music)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ece_BduOSV3t"
      },
      "source": [
        "no_of_timesteps = 32\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for note_ in new_music:\n",
        "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
        "        \n",
        "        #preparing input and output sequences\n",
        "        input_ = note_[i:i + no_of_timesteps]\n",
        "        output = note_[i + no_of_timesteps]\n",
        "        \n",
        "        x.append(input_)\n",
        "        y.append(output)\n",
        "        \n",
        "x=np.array(x)\n",
        "y=np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDLoy0TXSX6P"
      },
      "source": [
        "unique_x = list(set(x.ravel()))\n",
        "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oGv_KLqSaI2"
      },
      "source": [
        "#preparing input sequences\n",
        "x_seq=[]\n",
        "for i in x:\n",
        "    temp=[]\n",
        "    for j in i:\n",
        "        #assigning unique integer to every note\n",
        "        temp.append(x_note_to_int[j])\n",
        "    x_seq.append(temp)\n",
        "    \n",
        "x_seq = np.array(x_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db0-V57ASbtQ"
      },
      "source": [
        "unique_y = list(set(y))\n",
        "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
        "y_seq=np.array([y_note_to_int[i] for i in y])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrG9wfLLSduT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8SoSfnqSlU7",
        "outputId": "6f3c6b05-581c-4219-ba3b-89991d1807c3"
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "import keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "    \n",
        "#embedding layer\n",
        "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
        "\n",
        "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "    \n",
        "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "          \n",
        "#model.add(Conv1D(256,5,activation='relu'))    \n",
        "model.add(GlobalMaxPool1D())\n",
        "    \n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(len(unique_y), activation='softmax'))\n",
        "    \n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 32, 100)           4600      \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 46)                11822     \n",
            "=================================================================\n",
            "Total params: 224,742\n",
            "Trainable params: 224,742\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_7mqJHaSyWy"
      },
      "source": [
        "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ojiYj9HS3Rv",
        "outputId": "48f2a34b-a593-4928-c916-262f278999b3"
      },
      "source": [
        "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=500, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "69/69 [==============================] - 8s 90ms/step - loss: 3.6193 - val_loss: 3.2765\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.27653, saving model to best_model.h5\n",
            "Epoch 2/500\n",
            "69/69 [==============================] - 6s 82ms/step - loss: 3.1499 - val_loss: 3.1758\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.27653 to 3.17580, saving model to best_model.h5\n",
            "Epoch 3/500\n",
            "69/69 [==============================] - 6s 80ms/step - loss: 3.0406 - val_loss: 3.0859\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.17580 to 3.08586, saving model to best_model.h5\n",
            "Epoch 4/500\n",
            "69/69 [==============================] - 6s 81ms/step - loss: 2.9605 - val_loss: 3.0217\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.08586 to 3.02170, saving model to best_model.h5\n",
            "Epoch 5/500\n",
            "69/69 [==============================] - 6s 81ms/step - loss: 2.9041 - val_loss: 2.9809\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.02170 to 2.98091, saving model to best_model.h5\n",
            "Epoch 6/500\n",
            "69/69 [==============================] - 6s 82ms/step - loss: 2.8272 - val_loss: 2.9604\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.98091 to 2.96037, saving model to best_model.h5\n",
            "Epoch 7/500\n",
            "69/69 [==============================] - 6s 81ms/step - loss: 2.7863 - val_loss: 2.9289\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.96037 to 2.92889, saving model to best_model.h5\n",
            "Epoch 8/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.7339 - val_loss: 2.8923\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.92889 to 2.89226, saving model to best_model.h5\n",
            "Epoch 9/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.7222 - val_loss: 2.9064\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.89226\n",
            "Epoch 10/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.6763 - val_loss: 2.8565\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.89226 to 2.85650, saving model to best_model.h5\n",
            "Epoch 11/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.6262 - val_loss: 2.8754\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.85650\n",
            "Epoch 12/500\n",
            "69/69 [==============================] - 6s 82ms/step - loss: 2.6052 - val_loss: 2.8778\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.85650\n",
            "Epoch 13/500\n",
            "69/69 [==============================] - 6s 82ms/step - loss: 2.5747 - val_loss: 2.8418\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.85650 to 2.84185, saving model to best_model.h5\n",
            "Epoch 14/500\n",
            "69/69 [==============================] - 6s 81ms/step - loss: 2.5645 - val_loss: 2.8184\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.84185 to 2.81839, saving model to best_model.h5\n",
            "Epoch 15/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.5573 - val_loss: 2.8147\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.81839 to 2.81468, saving model to best_model.h5\n",
            "Epoch 16/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.4990 - val_loss: 2.8397\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 2.81468\n",
            "Epoch 17/500\n",
            "69/69 [==============================] - 6s 82ms/step - loss: 2.4835 - val_loss: 2.7977\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.81468 to 2.79770, saving model to best_model.h5\n",
            "Epoch 18/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.4524 - val_loss: 2.7955\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.79770 to 2.79549, saving model to best_model.h5\n",
            "Epoch 19/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.4443 - val_loss: 2.7815\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.79549 to 2.78152, saving model to best_model.h5\n",
            "Epoch 20/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 2.4187 - val_loss: 2.8066\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 2.78152\n",
            "Epoch 21/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.3874 - val_loss: 2.7894\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 2.78152\n",
            "Epoch 22/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.3569 - val_loss: 2.7920\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 2.78152\n",
            "Epoch 23/500\n",
            "69/69 [==============================] - 6s 82ms/step - loss: 2.3438 - val_loss: 2.7956\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 2.78152\n",
            "Epoch 24/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 2.2860 - val_loss: 2.8111\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 2.78152\n",
            "Epoch 25/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.2608 - val_loss: 2.8029\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 2.78152\n",
            "Epoch 26/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.2512 - val_loss: 2.8018\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 2.78152\n",
            "Epoch 27/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.2205 - val_loss: 2.8321\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.78152\n",
            "Epoch 28/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.2374 - val_loss: 2.8207\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 2.78152\n",
            "Epoch 29/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 2.1611 - val_loss: 2.8167\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 2.78152\n",
            "Epoch 30/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.1373 - val_loss: 2.8353\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 2.78152\n",
            "Epoch 31/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 2.1444 - val_loss: 2.8185\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 2.78152\n",
            "Epoch 32/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 2.1149 - val_loss: 2.8516\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 2.78152\n",
            "Epoch 33/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 2.0999 - val_loss: 2.8389\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 2.78152\n",
            "Epoch 34/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.0600 - val_loss: 2.8509\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 2.78152\n",
            "Epoch 35/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 2.0325 - val_loss: 2.8627\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 2.78152\n",
            "Epoch 36/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 2.0018 - val_loss: 2.8754\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 2.78152\n",
            "Epoch 37/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 2.0013 - val_loss: 2.8721\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 2.78152\n",
            "Epoch 38/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.9761 - val_loss: 2.8869\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 2.78152\n",
            "Epoch 39/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.9300 - val_loss: 2.8861\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 2.78152\n",
            "Epoch 40/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.9177 - val_loss: 2.9299\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 2.78152\n",
            "Epoch 41/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.9059 - val_loss: 2.9159\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 2.78152\n",
            "Epoch 42/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.8978 - val_loss: 2.9588\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 2.78152\n",
            "Epoch 43/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.8816 - val_loss: 2.9494\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 2.78152\n",
            "Epoch 44/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.8593 - val_loss: 2.9672\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 2.78152\n",
            "Epoch 45/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.8212 - val_loss: 2.9581\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 2.78152\n",
            "Epoch 46/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.8096 - val_loss: 2.9767\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 2.78152\n",
            "Epoch 47/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.7836 - val_loss: 2.9881\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 2.78152\n",
            "Epoch 48/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.7552 - val_loss: 3.0257\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 2.78152\n",
            "Epoch 49/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.7351 - val_loss: 3.0223\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 2.78152\n",
            "Epoch 50/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.7019 - val_loss: 3.0418\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 2.78152\n",
            "Epoch 51/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.7020 - val_loss: 3.0318\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 2.78152\n",
            "Epoch 52/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.6842 - val_loss: 3.0499\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 2.78152\n",
            "Epoch 53/500\n",
            "69/69 [==============================] - 6s 82ms/step - loss: 1.6596 - val_loss: 3.0832\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 2.78152\n",
            "Epoch 54/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.6597 - val_loss: 3.0462\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 2.78152\n",
            "Epoch 55/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.6499 - val_loss: 3.0676\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 2.78152\n",
            "Epoch 56/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.6211 - val_loss: 3.0990\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 2.78152\n",
            "Epoch 57/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 1.6027 - val_loss: 3.1200\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 2.78152\n",
            "Epoch 58/500\n",
            "69/69 [==============================] - 7s 99ms/step - loss: 1.5912 - val_loss: 3.1300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 2.78152\n",
            "Epoch 59/500\n",
            "69/69 [==============================] - 8s 118ms/step - loss: 1.5750 - val_loss: 3.1480\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 2.78152\n",
            "Epoch 60/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.5856 - val_loss: 3.1397\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 2.78152\n",
            "Epoch 61/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.5485 - val_loss: 3.1799\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 2.78152\n",
            "Epoch 62/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.5554 - val_loss: 3.1890\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 2.78152\n",
            "Epoch 63/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.5288 - val_loss: 3.1928\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 2.78152\n",
            "Epoch 64/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.5178 - val_loss: 3.2024\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 2.78152\n",
            "Epoch 65/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.4835 - val_loss: 3.2312\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 2.78152\n",
            "Epoch 66/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.4987 - val_loss: 3.2358\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 2.78152\n",
            "Epoch 67/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.4882 - val_loss: 3.2667\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 2.78152\n",
            "Epoch 68/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.4671 - val_loss: 3.2457\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 2.78152\n",
            "Epoch 69/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.4707 - val_loss: 3.2669\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 2.78152\n",
            "Epoch 70/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.4621 - val_loss: 3.2880\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 2.78152\n",
            "Epoch 71/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.4204 - val_loss: 3.2892\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 2.78152\n",
            "Epoch 72/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.4250 - val_loss: 3.2949\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 2.78152\n",
            "Epoch 73/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.4056 - val_loss: 3.3103\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 2.78152\n",
            "Epoch 74/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.4228 - val_loss: 3.3218\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 2.78152\n",
            "Epoch 75/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.4056 - val_loss: 3.3068\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 2.78152\n",
            "Epoch 76/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.4036 - val_loss: 3.3727\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 2.78152\n",
            "Epoch 77/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.3981 - val_loss: 3.3812\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 2.78152\n",
            "Epoch 78/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.3718 - val_loss: 3.3892\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 2.78152\n",
            "Epoch 79/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.3453 - val_loss: 3.3745\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 2.78152\n",
            "Epoch 80/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.3537 - val_loss: 3.3922\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 2.78152\n",
            "Epoch 81/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.3675 - val_loss: 3.3815\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 2.78152\n",
            "Epoch 82/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.3496 - val_loss: 3.4249\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 2.78152\n",
            "Epoch 83/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.3469 - val_loss: 3.4327\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 2.78152\n",
            "Epoch 84/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.3113 - val_loss: 3.4544\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 2.78152\n",
            "Epoch 85/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.3324 - val_loss: 3.4470\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 2.78152\n",
            "Epoch 86/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.3073 - val_loss: 3.4208\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 2.78152\n",
            "Epoch 87/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.2757 - val_loss: 3.4813\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 2.78152\n",
            "Epoch 88/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.2958 - val_loss: 3.4955\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 2.78152\n",
            "Epoch 89/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.2780 - val_loss: 3.4692\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 2.78152\n",
            "Epoch 90/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.2563 - val_loss: 3.4691\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 2.78152\n",
            "Epoch 91/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.2509 - val_loss: 3.4811\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 2.78152\n",
            "Epoch 92/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.2667 - val_loss: 3.5295\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 2.78152\n",
            "Epoch 93/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.2449 - val_loss: 3.5215\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 2.78152\n",
            "Epoch 94/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.2693 - val_loss: 3.5280\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 2.78152\n",
            "Epoch 95/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.2228 - val_loss: 3.5131\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 2.78152\n",
            "Epoch 96/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.2362 - val_loss: 3.5069\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 2.78152\n",
            "Epoch 97/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.2621 - val_loss: 3.5245\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 2.78152\n",
            "Epoch 98/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.2299 - val_loss: 3.5518\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 2.78152\n",
            "Epoch 99/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.2072 - val_loss: 3.5430\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 2.78152\n",
            "Epoch 100/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.1904 - val_loss: 3.5623\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 2.78152\n",
            "Epoch 101/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.1793 - val_loss: 3.5735\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 2.78152\n",
            "Epoch 102/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.2044 - val_loss: 3.5716\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 2.78152\n",
            "Epoch 103/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.1822 - val_loss: 3.5962\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 2.78152\n",
            "Epoch 104/500\n",
            "69/69 [==============================] - 6s 83ms/step - loss: 1.1887 - val_loss: 3.6352\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 2.78152\n",
            "Epoch 105/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.1598 - val_loss: 3.6697\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 2.78152\n",
            "Epoch 106/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.2016 - val_loss: 3.6447\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 2.78152\n",
            "Epoch 107/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.1691 - val_loss: 3.6424\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 2.78152\n",
            "Epoch 108/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.1462 - val_loss: 3.6391\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 2.78152\n",
            "Epoch 109/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.1955 - val_loss: 3.6607\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 2.78152\n",
            "Epoch 110/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.1779 - val_loss: 3.7032\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 2.78152\n",
            "Epoch 111/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.1658 - val_loss: 3.6832\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 2.78152\n",
            "Epoch 112/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.1527 - val_loss: 3.6943\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 2.78152\n",
            "Epoch 113/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.1325 - val_loss: 3.7109\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 2.78152\n",
            "Epoch 114/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.0895 - val_loss: 3.7480\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 2.78152\n",
            "Epoch 115/500\n",
            "69/69 [==============================] - 6s 84ms/step - loss: 1.1137 - val_loss: 3.7172\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 2.78152\n",
            "Epoch 116/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 1.1467 - val_loss: 3.6916\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 2.78152\n",
            "Epoch 117/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.0919 - val_loss: 3.7422\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 2.78152\n",
            "Epoch 118/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.1126 - val_loss: 3.7127\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 2.78152\n",
            "Epoch 119/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.1298 - val_loss: 3.7578\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 2.78152\n",
            "Epoch 120/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.1043 - val_loss: 3.7545\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 2.78152\n",
            "Epoch 121/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 1.0784 - val_loss: 3.7720\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 2.78152\n",
            "Epoch 122/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.1217 - val_loss: 3.7602\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 2.78152\n",
            "Epoch 123/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.1056 - val_loss: 3.7975\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 2.78152\n",
            "Epoch 124/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0881 - val_loss: 3.8075\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 2.78152\n",
            "Epoch 125/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.0900 - val_loss: 3.7802\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 2.78152\n",
            "Epoch 126/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0897 - val_loss: 3.7586\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 2.78152\n",
            "Epoch 127/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.0819 - val_loss: 3.7979\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 2.78152\n",
            "Epoch 128/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0913 - val_loss: 3.8387\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 2.78152\n",
            "Epoch 129/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0844 - val_loss: 3.8586\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 2.78152\n",
            "Epoch 130/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 1.0815 - val_loss: 3.8384\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 2.78152\n",
            "Epoch 131/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0745 - val_loss: 3.8528\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 2.78152\n",
            "Epoch 132/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0500 - val_loss: 3.8776\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 2.78152\n",
            "Epoch 133/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.0473 - val_loss: 3.8710\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 2.78152\n",
            "Epoch 134/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0485 - val_loss: 3.8644\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 2.78152\n",
            "Epoch 135/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0567 - val_loss: 3.8055\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 2.78152\n",
            "Epoch 136/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0858 - val_loss: 3.8651\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 2.78152\n",
            "Epoch 137/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0305 - val_loss: 3.9115\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 2.78152\n",
            "Epoch 138/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0181 - val_loss: 3.9100\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 2.78152\n",
            "Epoch 139/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 1.0136 - val_loss: 3.9599\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 2.78152\n",
            "Epoch 140/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 1.0290 - val_loss: 3.8948\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 2.78152\n",
            "Epoch 141/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0452 - val_loss: 3.9028\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 2.78152\n",
            "Epoch 142/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 1.0157 - val_loss: 3.9013\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 2.78152\n",
            "Epoch 143/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0137 - val_loss: 3.9282\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 2.78152\n",
            "Epoch 144/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0043 - val_loss: 3.9855\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 2.78152\n",
            "Epoch 145/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 1.0313 - val_loss: 3.9049\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 2.78152\n",
            "Epoch 146/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 1.0122 - val_loss: 3.9382\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 2.78152\n",
            "Epoch 147/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0267 - val_loss: 3.9824\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 2.78152\n",
            "Epoch 148/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0041 - val_loss: 3.9349\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 2.78152\n",
            "Epoch 149/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9649 - val_loss: 3.9553\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 2.78152\n",
            "Epoch 150/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 1.0156 - val_loss: 3.9775\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 2.78152\n",
            "Epoch 151/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.9623 - val_loss: 4.0079\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 2.78152\n",
            "Epoch 152/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9969 - val_loss: 3.9862\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 2.78152\n",
            "Epoch 153/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9897 - val_loss: 3.9872\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 2.78152\n",
            "Epoch 154/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9840 - val_loss: 3.9818\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 2.78152\n",
            "Epoch 155/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9929 - val_loss: 3.9715\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 2.78152\n",
            "Epoch 156/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9529 - val_loss: 3.9441\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 2.78152\n",
            "Epoch 157/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 1.0157 - val_loss: 3.9824\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 2.78152\n",
            "Epoch 158/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.9769 - val_loss: 4.0180\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 2.78152\n",
            "Epoch 159/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 1.0021 - val_loss: 3.9633\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 2.78152\n",
            "Epoch 160/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9971 - val_loss: 4.0114\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 2.78152\n",
            "Epoch 161/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.9688 - val_loss: 4.0226\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 2.78152\n",
            "Epoch 162/500\n",
            "69/69 [==============================] - 7s 106ms/step - loss: 0.9769 - val_loss: 4.0059\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 2.78152\n",
            "Epoch 163/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9811 - val_loss: 4.0147\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 2.78152\n",
            "Epoch 164/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9943 - val_loss: 4.0386\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 2.78152\n",
            "Epoch 165/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9551 - val_loss: 4.0212\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 2.78152\n",
            "Epoch 166/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9338 - val_loss: 4.0292\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 2.78152\n",
            "Epoch 167/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9485 - val_loss: 4.0137\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 2.78152\n",
            "Epoch 168/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9523 - val_loss: 4.0407\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 2.78152\n",
            "Epoch 169/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9369 - val_loss: 4.0973\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 2.78152\n",
            "Epoch 170/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9212 - val_loss: 4.0853\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 2.78152\n",
            "Epoch 171/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9333 - val_loss: 4.0842\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 2.78152\n",
            "Epoch 172/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.9292 - val_loss: 4.0728\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 2.78152\n",
            "Epoch 173/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9342 - val_loss: 4.0985\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 2.78152\n",
            "Epoch 174/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9453 - val_loss: 4.0834\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 2.78152\n",
            "Epoch 175/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9372 - val_loss: 4.1271\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 2.78152\n",
            "Epoch 176/500\n",
            "69/69 [==============================] - 6s 85ms/step - loss: 0.9547 - val_loss: 4.0977\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 2.78152\n",
            "Epoch 177/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9013 - val_loss: 4.1409\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 2.78152\n",
            "Epoch 178/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9381 - val_loss: 4.1517\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 2.78152\n",
            "Epoch 179/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9155 - val_loss: 4.1359\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 2.78152\n",
            "Epoch 180/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8908 - val_loss: 4.1631\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 2.78152\n",
            "Epoch 181/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9255 - val_loss: 4.1560\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 2.78152\n",
            "Epoch 182/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9062 - val_loss: 4.1841\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 2.78152\n",
            "Epoch 183/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.9171 - val_loss: 4.1815\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 2.78152\n",
            "Epoch 184/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9342 - val_loss: 4.1102\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 2.78152\n",
            "Epoch 185/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.9299 - val_loss: 4.1107\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 2.78152\n",
            "Epoch 186/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8965 - val_loss: 4.1696\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 2.78152\n",
            "Epoch 187/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9188 - val_loss: 4.1774\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 2.78152\n",
            "Epoch 188/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9208 - val_loss: 4.1948\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 2.78152\n",
            "Epoch 189/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8999 - val_loss: 4.2392\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 2.78152\n",
            "Epoch 190/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.9175 - val_loss: 4.1767\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 2.78152\n",
            "Epoch 191/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9318 - val_loss: 4.2109\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 2.78152\n",
            "Epoch 192/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.8702 - val_loss: 4.1915\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 2.78152\n",
            "Epoch 193/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.8853 - val_loss: 4.1662\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 2.78152\n",
            "Epoch 194/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9084 - val_loss: 4.2282\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 2.78152\n",
            "Epoch 195/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.8878 - val_loss: 4.2619\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 2.78152\n",
            "Epoch 196/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.9005 - val_loss: 4.2342\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 2.78152\n",
            "Epoch 197/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9010 - val_loss: 4.2206\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 2.78152\n",
            "Epoch 198/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9115 - val_loss: 4.2565\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 2.78152\n",
            "Epoch 199/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.9130 - val_loss: 4.2184\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 2.78152\n",
            "Epoch 200/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8760 - val_loss: 4.2000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 2.78152\n",
            "Epoch 201/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8534 - val_loss: 4.2334\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 2.78152\n",
            "Epoch 202/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8950 - val_loss: 4.2617\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 2.78152\n",
            "Epoch 203/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8655 - val_loss: 4.2252\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 2.78152\n",
            "Epoch 204/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8399 - val_loss: 4.2141\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 2.78152\n",
            "Epoch 205/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8608 - val_loss: 4.2570\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 2.78152\n",
            "Epoch 206/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.8744 - val_loss: 4.2870\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 2.78152\n",
            "Epoch 207/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8559 - val_loss: 4.2403\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 2.78152\n",
            "Epoch 208/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8635 - val_loss: 4.2702\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 2.78152\n",
            "Epoch 209/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8549 - val_loss: 4.2734\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 2.78152\n",
            "Epoch 210/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.8726 - val_loss: 4.2833\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 2.78152\n",
            "Epoch 211/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.8734 - val_loss: 4.2567\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 2.78152\n",
            "Epoch 212/500\n",
            "69/69 [==============================] - 6s 86ms/step - loss: 0.8309 - val_loss: 4.2627\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 2.78152\n",
            "Epoch 213/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8656 - val_loss: 4.2692\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 2.78152\n",
            "Epoch 214/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8753 - val_loss: 4.2600\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 2.78152\n",
            "Epoch 215/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8467 - val_loss: 4.2919\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 2.78152\n",
            "Epoch 216/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8548 - val_loss: 4.3223\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 2.78152\n",
            "Epoch 217/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8720 - val_loss: 4.3207\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 2.78152\n",
            "Epoch 218/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8438 - val_loss: 4.3238\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 2.78152\n",
            "Epoch 219/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8581 - val_loss: 4.3616\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 2.78152\n",
            "Epoch 220/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8642 - val_loss: 4.3186\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 2.78152\n",
            "Epoch 221/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8473 - val_loss: 4.3044\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 2.78152\n",
            "Epoch 222/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8292 - val_loss: 4.3166\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 2.78152\n",
            "Epoch 223/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.8566 - val_loss: 4.3200\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 2.78152\n",
            "Epoch 224/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.8439 - val_loss: 4.3007\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 2.78152\n",
            "Epoch 225/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.8128 - val_loss: 4.3589\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 2.78152\n",
            "Epoch 226/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.8469 - val_loss: 4.3884\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 2.78152\n",
            "Epoch 227/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.8594 - val_loss: 4.3235\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 2.78152\n",
            "Epoch 228/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.8465 - val_loss: 4.3599\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 2.78152\n",
            "Epoch 229/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.8512 - val_loss: 4.3337\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 2.78152\n",
            "Epoch 230/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8453 - val_loss: 4.3230\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 2.78152\n",
            "Epoch 231/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8479 - val_loss: 4.3337\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 2.78152\n",
            "Epoch 232/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8192 - val_loss: 4.3894\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 2.78152\n",
            "Epoch 233/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8133 - val_loss: 4.4102\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 2.78152\n",
            "Epoch 234/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8294 - val_loss: 4.3968\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 2.78152\n",
            "Epoch 235/500\n",
            "69/69 [==============================] - 6s 87ms/step - loss: 0.8355 - val_loss: 4.4014\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 2.78152\n",
            "Epoch 236/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8476 - val_loss: 4.3277\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 2.78152\n",
            "Epoch 237/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8147 - val_loss: 4.3866\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 2.78152\n",
            "Epoch 238/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8177 - val_loss: 4.3982\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 2.78152\n",
            "Epoch 239/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8182 - val_loss: 4.3700\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 2.78152\n",
            "Epoch 240/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8272 - val_loss: 4.3594\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 2.78152\n",
            "Epoch 241/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.8337 - val_loss: 4.3618\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 2.78152\n",
            "Epoch 242/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7980 - val_loss: 4.3519\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 2.78152\n",
            "Epoch 243/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8272 - val_loss: 4.4038\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 2.78152\n",
            "Epoch 244/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8176 - val_loss: 4.4199\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 2.78152\n",
            "Epoch 245/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.8355 - val_loss: 4.3859\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 2.78152\n",
            "Epoch 246/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8203 - val_loss: 4.3667\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 2.78152\n",
            "Epoch 247/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8368 - val_loss: 4.4627\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 2.78152\n",
            "Epoch 248/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8063 - val_loss: 4.3988\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 2.78152\n",
            "Epoch 249/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.8116 - val_loss: 4.4233\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 2.78152\n",
            "Epoch 250/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.8169 - val_loss: 4.3971\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 2.78152\n",
            "Epoch 251/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8191 - val_loss: 4.4279\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 2.78152\n",
            "Epoch 252/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8078 - val_loss: 4.4252\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 2.78152\n",
            "Epoch 253/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7961 - val_loss: 4.4904\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 2.78152\n",
            "Epoch 254/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8271 - val_loss: 4.4508\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 2.78152\n",
            "Epoch 255/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7632 - val_loss: 4.4665\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 2.78152\n",
            "Epoch 256/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.8254 - val_loss: 4.4229\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 2.78152\n",
            "Epoch 257/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7710 - val_loss: 4.4190\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 2.78152\n",
            "Epoch 258/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7905 - val_loss: 4.4388\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 2.78152\n",
            "Epoch 259/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7783 - val_loss: 4.4629\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 2.78152\n",
            "Epoch 260/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7780 - val_loss: 4.5338\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 2.78152\n",
            "Epoch 261/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7949 - val_loss: 4.5508\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 2.78152\n",
            "Epoch 262/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7982 - val_loss: 4.5204\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 2.78152\n",
            "Epoch 263/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7972 - val_loss: 4.5479\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 2.78152\n",
            "Epoch 264/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.8146 - val_loss: 4.4702\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 2.78152\n",
            "Epoch 265/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.8015 - val_loss: 4.5042\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 2.78152\n",
            "Epoch 266/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7649 - val_loss: 4.4764\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 2.78152\n",
            "Epoch 267/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7699 - val_loss: 4.4509\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 2.78152\n",
            "Epoch 268/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7690 - val_loss: 4.5420\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 2.78152\n",
            "Epoch 269/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7870 - val_loss: 4.5098\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 2.78152\n",
            "Epoch 270/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7497 - val_loss: 4.4898\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 2.78152\n",
            "Epoch 271/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7796 - val_loss: 4.5150\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 2.78152\n",
            "Epoch 272/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7407 - val_loss: 4.5636\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 2.78152\n",
            "Epoch 273/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7830 - val_loss: 4.5434\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 2.78152\n",
            "Epoch 274/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.8000 - val_loss: 4.4713\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 2.78152\n",
            "Epoch 275/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.7701 - val_loss: 4.5392\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 2.78152\n",
            "Epoch 276/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.7622 - val_loss: 4.5991\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 2.78152\n",
            "Epoch 277/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.8099 - val_loss: 4.5229\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 2.78152\n",
            "Epoch 278/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7617 - val_loss: 4.5592\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 2.78152\n",
            "Epoch 279/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7497 - val_loss: 4.5686\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 2.78152\n",
            "Epoch 280/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7917 - val_loss: 4.5295\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 2.78152\n",
            "Epoch 281/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7844 - val_loss: 4.5657\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 2.78152\n",
            "Epoch 282/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7612 - val_loss: 4.5515\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 2.78152\n",
            "Epoch 283/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7778 - val_loss: 4.5504\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 2.78152\n",
            "Epoch 284/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7675 - val_loss: 4.5281\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 2.78152\n",
            "Epoch 285/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7615 - val_loss: 4.4940\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 2.78152\n",
            "Epoch 286/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.7389 - val_loss: 4.6078\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 2.78152\n",
            "Epoch 287/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7723 - val_loss: 4.5863\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 2.78152\n",
            "Epoch 288/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.7453 - val_loss: 4.5578\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 2.78152\n",
            "Epoch 289/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7561 - val_loss: 4.5413\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 2.78152\n",
            "Epoch 290/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.7789 - val_loss: 4.5156\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 2.78152\n",
            "Epoch 291/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7439 - val_loss: 4.6115\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 2.78152\n",
            "Epoch 292/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7720 - val_loss: 4.5687\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 2.78152\n",
            "Epoch 293/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7813 - val_loss: 4.5309\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 2.78152\n",
            "Epoch 294/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7640 - val_loss: 4.5704\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 2.78152\n",
            "Epoch 295/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7311 - val_loss: 4.5892\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 2.78152\n",
            "Epoch 296/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7417 - val_loss: 4.6466\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 2.78152\n",
            "Epoch 297/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7659 - val_loss: 4.5668\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 2.78152\n",
            "Epoch 298/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.7541 - val_loss: 4.5784\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 2.78152\n",
            "Epoch 299/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7491 - val_loss: 4.5888\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 2.78152\n",
            "Epoch 300/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7491 - val_loss: 4.6236\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 2.78152\n",
            "Epoch 301/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7345 - val_loss: 4.6087\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 2.78152\n",
            "Epoch 302/500\n",
            "69/69 [==============================] - 7s 94ms/step - loss: 0.7395 - val_loss: 4.6235\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 2.78152\n",
            "Epoch 303/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7380 - val_loss: 4.6208\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 2.78152\n",
            "Epoch 304/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7834 - val_loss: 4.6044\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 2.78152\n",
            "Epoch 305/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7770 - val_loss: 4.5903\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 2.78152\n",
            "Epoch 306/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7871 - val_loss: 4.6028\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 2.78152\n",
            "Epoch 307/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7251 - val_loss: 4.6034\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 2.78152\n",
            "Epoch 308/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7532 - val_loss: 4.5745\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 2.78152\n",
            "Epoch 309/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7333 - val_loss: 4.6122\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 2.78152\n",
            "Epoch 310/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7229 - val_loss: 4.6449\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 2.78152\n",
            "Epoch 311/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7307 - val_loss: 4.6243\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 2.78152\n",
            "Epoch 312/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7191 - val_loss: 4.6785\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 2.78152\n",
            "Epoch 313/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7440 - val_loss: 4.6644\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 2.78152\n",
            "Epoch 314/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7594 - val_loss: 4.6268\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 2.78152\n",
            "Epoch 315/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7466 - val_loss: 4.5704\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 2.78152\n",
            "Epoch 316/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7300 - val_loss: 4.6566\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 2.78152\n",
            "Epoch 317/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7212 - val_loss: 4.6517\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 2.78152\n",
            "Epoch 318/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7151 - val_loss: 4.6663\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 2.78152\n",
            "Epoch 319/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7398 - val_loss: 4.6551\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 2.78152\n",
            "Epoch 320/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.7245 - val_loss: 4.6712\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 2.78152\n",
            "Epoch 321/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7580 - val_loss: 4.6270\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 2.78152\n",
            "Epoch 322/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7303 - val_loss: 4.6372\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 2.78152\n",
            "Epoch 323/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7465 - val_loss: 4.6157\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 2.78152\n",
            "Epoch 324/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7421 - val_loss: 4.6144\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 2.78152\n",
            "Epoch 325/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7456 - val_loss: 4.6745\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 2.78152\n",
            "Epoch 326/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7378 - val_loss: 4.5996\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 2.78152\n",
            "Epoch 327/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7133 - val_loss: 4.5943\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 2.78152\n",
            "Epoch 328/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7382 - val_loss: 4.6043\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 2.78152\n",
            "Epoch 329/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7246 - val_loss: 4.6462\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 2.78152\n",
            "Epoch 330/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7184 - val_loss: 4.7124\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 2.78152\n",
            "Epoch 331/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7018 - val_loss: 4.7114\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 2.78152\n",
            "Epoch 332/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7324 - val_loss: 4.6913\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 2.78152\n",
            "Epoch 333/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7247 - val_loss: 4.6533\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 2.78152\n",
            "Epoch 334/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7107 - val_loss: 4.6492\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 2.78152\n",
            "Epoch 335/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7437 - val_loss: 4.6046\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 2.78152\n",
            "Epoch 336/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7208 - val_loss: 4.6492\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 2.78152\n",
            "Epoch 337/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6941 - val_loss: 4.6472\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 2.78152\n",
            "Epoch 338/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7433 - val_loss: 4.6871\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 2.78152\n",
            "Epoch 339/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7118 - val_loss: 4.7016\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 2.78152\n",
            "Epoch 340/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7132 - val_loss: 4.7585\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 2.78152\n",
            "Epoch 341/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7075 - val_loss: 4.7395\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 2.78152\n",
            "Epoch 342/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7026 - val_loss: 4.7188\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 2.78152\n",
            "Epoch 343/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7053 - val_loss: 4.7756\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 2.78152\n",
            "Epoch 344/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6893 - val_loss: 4.6870\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 2.78152\n",
            "Epoch 345/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7329 - val_loss: 4.7215\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 2.78152\n",
            "Epoch 346/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7087 - val_loss: 4.7458\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 2.78152\n",
            "Epoch 347/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7043 - val_loss: 4.7312\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 2.78152\n",
            "Epoch 348/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6936 - val_loss: 4.7220\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 2.78152\n",
            "Epoch 349/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6898 - val_loss: 4.7002\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 2.78152\n",
            "Epoch 350/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7211 - val_loss: 4.7668\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 2.78152\n",
            "Epoch 351/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7115 - val_loss: 4.6985\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 2.78152\n",
            "Epoch 352/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6945 - val_loss: 4.7021\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 2.78152\n",
            "Epoch 353/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7310 - val_loss: 4.7100\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 2.78152\n",
            "Epoch 354/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7260 - val_loss: 4.7089\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 2.78152\n",
            "Epoch 355/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6987 - val_loss: 4.6853\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 2.78152\n",
            "Epoch 356/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7077 - val_loss: 4.7294\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 2.78152\n",
            "Epoch 357/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7054 - val_loss: 4.6987\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 2.78152\n",
            "Epoch 358/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7138 - val_loss: 4.6981\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 2.78152\n",
            "Epoch 359/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6979 - val_loss: 4.7592\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 2.78152\n",
            "Epoch 360/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6829 - val_loss: 4.7521\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 2.78152\n",
            "Epoch 361/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.7158 - val_loss: 4.8124\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 2.78152\n",
            "Epoch 362/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6711 - val_loss: 4.7365\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 2.78152\n",
            "Epoch 363/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.7062 - val_loss: 4.7887\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 2.78152\n",
            "Epoch 364/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.6967 - val_loss: 4.7807\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 2.78152\n",
            "Epoch 365/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7029 - val_loss: 4.7628\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 2.78152\n",
            "Epoch 366/500\n",
            "69/69 [==============================] - 6s 89ms/step - loss: 0.7090 - val_loss: 4.7227\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 2.78152\n",
            "Epoch 367/500\n",
            "69/69 [==============================] - 6s 88ms/step - loss: 0.6922 - val_loss: 4.7177\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 2.78152\n",
            "Epoch 368/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7102 - val_loss: 4.7223\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 2.78152\n",
            "Epoch 369/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6806 - val_loss: 4.7715\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 2.78152\n",
            "Epoch 370/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6935 - val_loss: 4.7676\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 2.78152\n",
            "Epoch 371/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6842 - val_loss: 4.7188\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 2.78152\n",
            "Epoch 372/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6828 - val_loss: 4.7450\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 2.78152\n",
            "Epoch 373/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6833 - val_loss: 4.7758\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 2.78152\n",
            "Epoch 374/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6799 - val_loss: 4.8312\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 2.78152\n",
            "Epoch 375/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6873 - val_loss: 4.7892\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 2.78152\n",
            "Epoch 376/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6905 - val_loss: 4.8606\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 2.78152\n",
            "Epoch 377/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6984 - val_loss: 4.8310\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 2.78152\n",
            "Epoch 378/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.7029 - val_loss: 4.7700\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 2.78152\n",
            "Epoch 379/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6673 - val_loss: 4.7945\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 2.78152\n",
            "Epoch 380/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6742 - val_loss: 4.8195\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 2.78152\n",
            "Epoch 381/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6784 - val_loss: 4.8377\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 2.78152\n",
            "Epoch 382/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6816 - val_loss: 4.8416\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 2.78152\n",
            "Epoch 383/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6897 - val_loss: 4.8406\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 2.78152\n",
            "Epoch 384/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6878 - val_loss: 4.8164\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 2.78152\n",
            "Epoch 385/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6680 - val_loss: 4.8566\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 2.78152\n",
            "Epoch 386/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6873 - val_loss: 4.8349\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 2.78152\n",
            "Epoch 387/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6527 - val_loss: 4.8308\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 2.78152\n",
            "Epoch 388/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6652 - val_loss: 4.8176\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 2.78152\n",
            "Epoch 389/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6861 - val_loss: 4.8047\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 2.78152\n",
            "Epoch 390/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6822 - val_loss: 4.7901\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 2.78152\n",
            "Epoch 391/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6667 - val_loss: 4.8220\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 2.78152\n",
            "Epoch 392/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6637 - val_loss: 4.8807\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 2.78152\n",
            "Epoch 393/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6665 - val_loss: 4.8469\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 2.78152\n",
            "Epoch 394/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6770 - val_loss: 4.8417\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 2.78152\n",
            "Epoch 395/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6757 - val_loss: 4.7848\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 2.78152\n",
            "Epoch 396/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6841 - val_loss: 4.8142\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 2.78152\n",
            "Epoch 397/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6760 - val_loss: 4.8022\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 2.78152\n",
            "Epoch 398/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6953 - val_loss: 4.8042\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 2.78152\n",
            "Epoch 399/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6846 - val_loss: 4.8276\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 2.78152\n",
            "Epoch 400/500\n",
            "69/69 [==============================] - 6s 90ms/step - loss: 0.6550 - val_loss: 4.8566\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 2.78152\n",
            "Epoch 401/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6818 - val_loss: 4.8865\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 2.78152\n",
            "Epoch 402/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6609 - val_loss: 4.8245\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 2.78152\n",
            "Epoch 403/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6928 - val_loss: 4.7905\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 2.78152\n",
            "Epoch 404/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.7059 - val_loss: 4.8053\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 2.78152\n",
            "Epoch 405/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6772 - val_loss: 4.8094\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 2.78152\n",
            "Epoch 406/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6476 - val_loss: 4.8158\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 2.78152\n",
            "Epoch 407/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6627 - val_loss: 4.8434\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 2.78152\n",
            "Epoch 408/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6773 - val_loss: 4.8193\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 2.78152\n",
            "Epoch 409/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6357 - val_loss: 4.8565\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 2.78152\n",
            "Epoch 410/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6538 - val_loss: 4.8284\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 2.78152\n",
            "Epoch 411/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6633 - val_loss: 4.8808\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 2.78152\n",
            "Epoch 412/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6660 - val_loss: 4.8191\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 2.78152\n",
            "Epoch 413/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6405 - val_loss: 4.8849\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 2.78152\n",
            "Epoch 414/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6651 - val_loss: 4.8334\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 2.78152\n",
            "Epoch 415/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6776 - val_loss: 4.8126\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 2.78152\n",
            "Epoch 416/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6523 - val_loss: 4.7732\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 2.78152\n",
            "Epoch 417/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6853 - val_loss: 4.8362\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 2.78152\n",
            "Epoch 418/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6838 - val_loss: 4.8354\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 2.78152\n",
            "Epoch 419/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6500 - val_loss: 4.9297\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 2.78152\n",
            "Epoch 420/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6891 - val_loss: 4.8750\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 2.78152\n",
            "Epoch 421/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6680 - val_loss: 4.9127\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 2.78152\n",
            "Epoch 422/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6514 - val_loss: 4.9120\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 2.78152\n",
            "Epoch 423/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6430 - val_loss: 4.8968\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 2.78152\n",
            "Epoch 424/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6566 - val_loss: 4.8333\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 2.78152\n",
            "Epoch 425/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6624 - val_loss: 4.7931\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 2.78152\n",
            "Epoch 426/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6729 - val_loss: 4.8458\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 2.78152\n",
            "Epoch 427/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6863 - val_loss: 4.8756\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 2.78152\n",
            "Epoch 428/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6389 - val_loss: 4.9186\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 2.78152\n",
            "Epoch 429/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6679 - val_loss: 4.9014\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 2.78152\n",
            "Epoch 430/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6623 - val_loss: 4.9323\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 2.78152\n",
            "Epoch 431/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6674 - val_loss: 4.8927\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 2.78152\n",
            "Epoch 432/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6035 - val_loss: 4.9419\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 2.78152\n",
            "Epoch 433/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6656 - val_loss: 4.9283\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 2.78152\n",
            "Epoch 434/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6559 - val_loss: 4.8919\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 2.78152\n",
            "Epoch 435/500\n",
            "69/69 [==============================] - 7s 98ms/step - loss: 0.6580 - val_loss: 4.9464\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 2.78152\n",
            "Epoch 436/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6554 - val_loss: 4.9088\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 2.78152\n",
            "Epoch 437/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6610 - val_loss: 4.9134\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 2.78152\n",
            "Epoch 438/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6615 - val_loss: 4.9245\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 2.78152\n",
            "Epoch 439/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6588 - val_loss: 4.9298\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 2.78152\n",
            "Epoch 440/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6560 - val_loss: 4.9329\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 2.78152\n",
            "Epoch 441/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6430 - val_loss: 4.9069\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 2.78152\n",
            "Epoch 442/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6347 - val_loss: 4.9974\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 2.78152\n",
            "Epoch 443/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6448 - val_loss: 4.9290\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 2.78152\n",
            "Epoch 444/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6400 - val_loss: 4.9435\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 2.78152\n",
            "Epoch 445/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6571 - val_loss: 4.8731\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 2.78152\n",
            "Epoch 446/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6203 - val_loss: 4.9497\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 2.78152\n",
            "Epoch 447/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6591 - val_loss: 4.9460\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 2.78152\n",
            "Epoch 448/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6367 - val_loss: 4.9785\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 2.78152\n",
            "Epoch 449/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6114 - val_loss: 5.0269\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 2.78152\n",
            "Epoch 450/500\n",
            "69/69 [==============================] - 7s 94ms/step - loss: 0.6705 - val_loss: 4.8749\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 2.78152\n",
            "Epoch 451/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.6512 - val_loss: 4.8744\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 2.78152\n",
            "Epoch 452/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6340 - val_loss: 4.9644\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 2.78152\n",
            "Epoch 453/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6258 - val_loss: 4.9430\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 2.78152\n",
            "Epoch 454/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6623 - val_loss: 4.9590\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 2.78152\n",
            "Epoch 455/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6456 - val_loss: 4.9888\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 2.78152\n",
            "Epoch 456/500\n",
            "69/69 [==============================] - 7s 96ms/step - loss: 0.6599 - val_loss: 4.8867\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 2.78152\n",
            "Epoch 457/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6166 - val_loss: 4.9310\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 2.78152\n",
            "Epoch 458/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6618 - val_loss: 4.9634\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 2.78152\n",
            "Epoch 459/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6143 - val_loss: 4.9225\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 2.78152\n",
            "Epoch 460/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6263 - val_loss: 4.9306\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 2.78152\n",
            "Epoch 461/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6391 - val_loss: 4.9645\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 2.78152\n",
            "Epoch 462/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6288 - val_loss: 5.0302\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 2.78152\n",
            "Epoch 463/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6434 - val_loss: 4.9639\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 2.78152\n",
            "Epoch 464/500\n",
            "69/69 [==============================] - 7s 96ms/step - loss: 0.6076 - val_loss: 5.0015\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 2.78152\n",
            "Epoch 465/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6176 - val_loss: 4.9796\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 2.78152\n",
            "Epoch 466/500\n",
            "69/69 [==============================] - 7s 96ms/step - loss: 0.6264 - val_loss: 5.0104\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 2.78152\n",
            "Epoch 467/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6061 - val_loss: 4.9964\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 2.78152\n",
            "Epoch 468/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6124 - val_loss: 4.9789\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 2.78152\n",
            "Epoch 469/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.6499 - val_loss: 5.0074\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 2.78152\n",
            "Epoch 470/500\n",
            "69/69 [==============================] - 7s 96ms/step - loss: 0.6552 - val_loss: 4.9173\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 2.78152\n",
            "Epoch 471/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6559 - val_loss: 4.9021\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 2.78152\n",
            "Epoch 472/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6212 - val_loss: 4.9822\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 2.78152\n",
            "Epoch 473/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6622 - val_loss: 5.0105\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 2.78152\n",
            "Epoch 474/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6381 - val_loss: 4.9617\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 2.78152\n",
            "Epoch 475/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6306 - val_loss: 5.0351\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 2.78152\n",
            "Epoch 476/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6211 - val_loss: 4.9897\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 2.78152\n",
            "Epoch 477/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6440 - val_loss: 5.0230\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 2.78152\n",
            "Epoch 478/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.6311 - val_loss: 4.9974\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 2.78152\n",
            "Epoch 479/500\n",
            "69/69 [==============================] - 7s 97ms/step - loss: 0.6228 - val_loss: 4.9601\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 2.78152\n",
            "Epoch 480/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6221 - val_loss: 5.0446\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 2.78152\n",
            "Epoch 481/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.6313 - val_loss: 4.9996\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 2.78152\n",
            "Epoch 482/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6471 - val_loss: 4.9656\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 2.78152\n",
            "Epoch 483/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6345 - val_loss: 5.0108\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 2.78152\n",
            "Epoch 484/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6339 - val_loss: 4.9639\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 2.78152\n",
            "Epoch 485/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6285 - val_loss: 4.9633\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 2.78152\n",
            "Epoch 486/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6072 - val_loss: 4.9613\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 2.78152\n",
            "Epoch 487/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6160 - val_loss: 5.0049\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 2.78152\n",
            "Epoch 488/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6415 - val_loss: 4.9595\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 2.78152\n",
            "Epoch 489/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6000 - val_loss: 5.0381\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 2.78152\n",
            "Epoch 490/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6330 - val_loss: 5.0343\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 2.78152\n",
            "Epoch 491/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6189 - val_loss: 5.0733\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 2.78152\n",
            "Epoch 492/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6364 - val_loss: 5.0033\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 2.78152\n",
            "Epoch 493/500\n",
            "69/69 [==============================] - 6s 91ms/step - loss: 0.6023 - val_loss: 5.0395\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 2.78152\n",
            "Epoch 494/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6125 - val_loss: 4.9942\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 2.78152\n",
            "Epoch 495/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6289 - val_loss: 4.9616\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 2.78152\n",
            "Epoch 496/500\n",
            "69/69 [==============================] - 6s 92ms/step - loss: 0.6370 - val_loss: 4.9983\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 2.78152\n",
            "Epoch 497/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6077 - val_loss: 5.0058\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 2.78152\n",
            "Epoch 498/500\n",
            "69/69 [==============================] - 6s 94ms/step - loss: 0.6348 - val_loss: 4.9953\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 2.78152\n",
            "Epoch 499/500\n",
            "69/69 [==============================] - 6s 93ms/step - loss: 0.6354 - val_loss: 4.9735\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 2.78152\n",
            "Epoch 500/500\n",
            "69/69 [==============================] - 7s 95ms/step - loss: 0.6161 - val_loss: 5.0264\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 2.78152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Ie2uK3S6cB"
      },
      "source": [
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkRC64rsS-fl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab984365-e1b9-4d2c-a5eb-253f7abec28b"
      },
      "source": [
        "import random\n",
        "ind = np.random.randint(0,len(x_val)-1)\n",
        "\n",
        "random_music = x_val[ind]\n",
        "\n",
        "predictions=[]\n",
        "for i in range(10):\n",
        "\n",
        "    random_music = random_music.reshape(1,no_of_timesteps)\n",
        "\n",
        "    prob  = model.predict(random_music)[0]\n",
        "    y_pred= np.argmax(prob,axis=0)\n",
        "    predictions.append(y_pred)\n",
        "\n",
        "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
        "    random_music = random_music[1:]\n",
        "    \n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[38, 38, 38, 38, 38, 38, 38, 38, 38, 38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmiDsN9US_kX"
      },
      "source": [
        "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
        "predicted_notes = [x_int_to_note[i] for i in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqbOEzq-TBFf"
      },
      "source": [
        "def convert_to_midi(prediction_output):\n",
        "   \n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        \n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                \n",
        "                cn=int(current_note)\n",
        "                new_note = note.Note(cn)\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "                \n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "            \n",
        "        # pattern is a note\n",
        "        else:\n",
        "            \n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 1\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='music4.mid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PfeYClPTDY2"
      },
      "source": [
        "convert_to_midi(predicted_notes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}